it is named after ronald fisher using the kernel trick lda is implicitly performed in a new feature space which allows non linear mappings to be learned intuitively the idea of lda is to find a projection where class separation is maximized given two sets of labeled data formula 1 and formula 2 define the class means formula 3 and formula 4 to be where formula 6 is the number of examples of class formula 7 the goal of linear discriminant analysis is to give a large separation of the class means while also keeping the in class variance small this is formulated as maximizing with respect to formula 8 the following ratio where formula 10 is the between class covariance matrix and formula 11 is the total within class covariance matrix the maximum of the above ratio is attained at as can be shown by the lagrange multiplier method sketch of proof maximizing formula 14 is equivalent to maximizing subject to this in turn is equivalent to maximizing formula 17 where formula 18 is the lagrange multiplier at the maximum the derivatives of formula 19 with respect to formula 8 and formula 18 must be zero taking formula 22