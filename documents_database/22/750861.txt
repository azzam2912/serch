the choice of logarithmic base in the following formulae determines the unit of information entropy that is used the most common unit of information is the bit based on the binary logarithm other units include the nat based on the natural logarithm and the hartley based on the base 10 or common logarithm in what follows an expression of the form formula 1 is considered by convention to be equal to zero whenever formula 2 is zero this is justified because formula 3 for any logarithmic base shannon derived a measure of information content called the self information or surprisal of a message formula 4 where formula 6 is the probability that message formula 4 is chosen from all possible choices in the message space formula 8 the base of the logarithm only affects a scaling factor and consequently the units in which the measured information content is expressed if the logarithm is base 2 the measure of information is expressed in units of bits information is transferred from a source to a recipient only if the recipient of the information did not already have the information to begin with messages that convey information that is certain to happen and