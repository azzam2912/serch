the term is typically used in artificial and biological neural network research in a computational neural network a vector or set of inputs formula 1 and outputs formula 2 or pre and post synaptic neurons respectively are interconnected with synaptic weights represented by the matrix formula 3 where for a linear neuron the synaptic weight is changed by using a learning rule the most basic of which is hebb s rule which is usually stated in biological terms as neurons that fire together wire together computationally this means that if a large signal from one of the input neurons results in a large signal from one of the output neurons then the synaptic weight between those two neurons will increase the rule is unstable however and is typically modified using such variations as oja s rule radial basis functions or the backpropagation algorithm for biological networks the effect of synaptic weights is not as simple as for linear neurons or hebbian learning however biophysical models such as bcm theory have seen some success in mathematically describing these networks in the mammalian central nervous system signal transmission is carried out by interconnected networks of nerve cells or neurons for the basic