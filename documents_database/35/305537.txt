backpropagation efficiently computes the gradient of the loss function with respect to the weights of the network for a single input output example this makes it feasible to use gradient methods for training multi layer networks updating weights to minimize loss commonly one uses gradient descent or variants such as stochastic gradient descent the backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule iterating backwards one layer at a time from the last layer to avoid redundant calculations of intermediate terms in the chain rule this is an example of dynamic programming the term backpropagation strictly refers only to the algorithm for computing the gradient but it is often used loosely to refer to the entire learning algorithm also including how the gradient is used such as by stochastic gradient descent backpropagation generalizes the gradient computation in the delta rule which is the single layer version of backpropagation and is in turn generalized by automatic differentiation where backpropagation is a special case of reverse accumulation or reverse mode the term backpropagation and its general use in neural networks was announced in then elaborated and popularized in but the technique