it can be used to calculate the informational difference between measurements the metric is interesting in several respects by chentsov s theorem the fisher information metric on statistical models is the only riemannian metric up to rescaling that is invariant under sufficient statistics it can also be understood to be the infinitesimal form of the relative entropy i e the kullback leibler divergence specifically it is the hessian of the divergence alternately it can be understood as the metric induced by the flat space euclidean metric after appropriate changes of variable when extended to complex projective hilbert space it becomes the fubini study metric when written in terms of mixed states it is the quantum bures metric considered purely as a matrix it is known as the fisher information matrix considered as a measurement technique where it is used to estimate hidden parameters in terms of observed random variables it is known as the observed information given a statistical manifold with coordinates formula 1 one writes formula 2 for the probability distribution as a function of formula 3 here formula 4 is drawn from the value space r for a discrete or continuous random variable x the probability is normalized