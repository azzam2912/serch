here information is measured in shannons nats or hartleys the entropy of formula 1 conditioned on formula 2 is written as formula 5 the conditional entropy of formula 1 given formula 2 is defined as where formula 8 and formula 9 denote the support sets of formula 2 and formula 1 note it is conventioned that the expressions formula 12 and formula 13 for fixed formula 14 should be treated as being equal to zero this is because formula 15 and formula 16 let formula 17 be the entropy of the discrete random variable formula 1 conditioned on the discrete random variable formula 2 taking a certain value formula 20 denote the support sets of formula 2 and formula 1 by formula 8 and formula 9 let formula 1 have probability mass function formula 26 the unconditional entropy of formula 1 is calculated as formula 28 i e where formula 30 is the information content of the outcome of formula 1 taking the value formula 32 the entropy of formula 1 conditioned on formula 2 taking the value formula 20 is defined analogously by conditional expectation formula 5 is the result of averaging formula 17 over all possible values formula