it is widely considered the most effective method of smoothing due to its use of absolute discounting by subtracting a fixed value from the probability s lower order terms to omit n grams with lower frequencies this approach has been considered equally effective for both higher and lower order n grams the method was proposed in a 1994 paper by reinhard kneser ute essen and a common example that illustrates the concept behind this method is the frequency of the bigram san francisco if it appears several times in a training corpus the frequency of the unigram francisco will also be high relying on only the unigram frequency to predict the frequencies of n grams leads to skewed results however kneser ney smoothing corrects this by considering the frequency of the unigram in relation to possible words preceding it let formula 1 be the number of occurrences of the word formula 2 followed by the word formula 3 in the corpus the equation for bigram probabilities is as follows formula 4 where the unigram probability formula 5 depends on how likely it is to see the word formula 6 in an unfamiliar context which is estimated as the number of