rnns have been successful for instance in learning sequence and tree structures in natural language processing mainly phrase and sentence continuous representations based on word embedding rnns have first been introduced to learn distributed representations of structure such as logical terms models and general frameworks have been developed in further works since the 1990s in the most simple architecture nodes are combined into parents using a weight matrix that is shared across the whole network and a non linearity such as tanh if c and c are n dimensional vector representation of nodes their parent will also be an n dimensional vector calculated as formula 1 where w is a learned formula 2 weight matrix this architecture with a few improvements has been used for successfully parsing natural scenes and for syntactic parsing of natural language sentences reccc is a constructive neural network approach to deal with tree domains with pioneering applications to chemistry and extension to directed acyclic graphs a framework for unsupervised rnn has been introduced in 2004 recursive neural tensor networks use one tensor based composition function for all nodes in the tree typically stochastic gradient descent sgd is used to train the network the gradient is