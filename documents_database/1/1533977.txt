to fix this problem we vary the width of the kernel in different regions of the sample space there are two methods of doing this balloon and pointwise estimation in a balloon estimator the kernel width is varied depending on the location of the test point in a pointwise estimator the kernel width is varied depending on the location of the sample for multivariate estimators the parameter h can be generalized to vary not just the size but also the shape of the kernel this more complicated approach will not be covered here a common method of varying the kernel width is to make it inversely proportional to the density at the test point where k is a constant if we back substitute the estimated pdf and assuming a gaussian kernel function we can show that w is a constant a similar derivation holds for any kernel whose normalising function is of the order although with a different constant factor in place of the term this produces a generalization of the k nearest neighbour algorithm that is a uniform kernel function will return the knn technique there are two components to the error a variance term and a bias term