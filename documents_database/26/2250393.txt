query understanding methods generally take place before the search engine retrieves and ranks results it is related to natural language processing but specifically focused on the understanding of search queries query understanding is at the heart of technologies like amazon alexa apple s siri google assistant ibm s watson and microsoft s cortana tokenization is the process of breaking up a text string into words or other meaningful elements called tokens typically tokenization occurs at the word level however it is sometimes difficult to define what is meant by a word often a tokenizer relies on simple heuristics such as splitting the string on punctuation and whitespace characters tokenization is more challenging in languages without spaces between words such as chinese and japanese tokenizing text in these languages requires the use of word segmentation algorithms spelling correction is the process of automatically detecting and correcting spelling errors in search queries most spelling correction algorithms are based on a language model which determines the a priori probability of an intended query and an error model typically a noisy channel model which determines the probability of a particular misspelling given an intended query many but not all language inflect words to reflect