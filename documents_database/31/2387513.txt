like stochastic gradient descent sgld is an iterative optimization algorithm which introduces additional noise to the stochastic gradient estimator used in sgd to optimize a differentiable objective function unlike traditional sgd sgld can be used for bayesian learning since the method produces samples from a posterior distribution of parameters based on available data first described by welling and teh in 2011 the method has applications in many contexts which require optimization and is most notably applied in machine learning problems given some parameter vector formula 1 its prior distribution formula 2 and a set of data points formula 3 stochastic gradient langevin dynamics samples from the posterior distribution formula 4 by updating the chain formula 5 where formula 6 is a positive integer formula 7 is gaussian noise formula 8 is the likelihood of the data given the parameter vector formula 1 and our step sizes formula 10satisfy the following conditions formula 11 for early iterations of the algorithm each parameter update mimics stochastic gradient descent however as the algorithm approaches a local minima or maxima the gradient shrinks to zero and the chain produces samples surrounding the maximum a posteriori mode allowing for posterior inference this process generates approximate