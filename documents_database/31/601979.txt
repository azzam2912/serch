it is also known as information radius irad or total divergence to the average it is based on the kullback leibler divergence with some notable and useful differences including that it is symmetric and it always has a finite value the square root of the jensen shannon divergence is a metric often referred to as jensen shannon distance consider the set formula 1 of probability distributions where a is a set provided with some algebra of measurable subsets in particular we can take a to be a finite or countable set with all subsets being measurable the jensen shannon divergence jsd formula 2 is a symmetrized and smoothed version of the kullback leibler divergence formula 3 it is defined by where formula 5 a generalization of the jensen shannon divergence using abstract means like the geometric or harmonic means instead of the arithmetic mean was recently proposed the geometric jensen shannon divergence or g jensen shannon divergence yields a closed form formula for gaussian distributions by taking the geometric mean a more general definition allowing for the comparison of more than two probability distributions is where formula 7 are weights that are selected for the probability distributions formula 8 and