in a controlled dynamical system the value function represents the optimal payoff of the system over the interval t t when started at the time t state variable x t x if the objective function represents some cost that is to be minimized the value function can be interpreted as the cost to finish the optimal program and is thus referred to as cost to go function in an economic context where the objective function usually represents utility the value function is conceptually equivalent to the indirect utility function in a problem of optimal control the value function is defined as the supremum of the objective function taken over the set of admissible controls given formula 1 a typical optimal control problem is subject to with initial state variable formula 4 the objective function formula 5 is to be maximized over all admissible controls formula 6 where formula 7 is a lebesgue measurable function from formula 8 to some prescribed arbitrary set in formula 9 the value function is then defined as if the optimal pair of control and state trajectories is formula 10 then formula 11 in economics the function formula 12 is called a policy function bellman s