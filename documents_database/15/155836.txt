applications include characterizing the relative shannon entropy in information systems randomness in continuous time series and information gain when comparing statistical models of inference in contrast to variation of information it is a distribution wise asymmetric measure and thus does not qualify as a statistical metric of spread it also does not satisfy the triangle inequality in the simple case a kullback leibler divergence of 0 indicates that the two distributions in question are identical in simplified terms it is a measure of surprise with diverse applications such as applied statistics fluid mechanics neuroscience and machine learning the kullback leibler divergence was introduced by solomon kullback and richard leibler in 1951 as the directed divergence between two distributions kullback preferred the term discrimination information the divergence is discussed in kullback s 1959 book information theory and statistics for discrete probability distributions formula 1 and formula 2 defined on the same probability space the kullback leibler divergence of formula 2 from formula 1 is defined to be which is equivalent to in other words it is the expectation of the logarithmic difference between the probabilities formula 1 and formula 2 where the expectation is taken using the probabilities formula 1 the