random projection methods are known for their power simplicity and low error rates when compared to other methods according to experimental results random projection preserves distances well but empirical results are sparse they have been applied to many natural language tasks under the name random indexing dimensionality reduction as the name suggests is reducing the number of random variables using various mathematical methods from statistics and machine learning dimensionality reduction is often used to reduce the problem of managing and manipulating large data sets dimensionality reduction techniques generally use linear transformations in determining the intrinsic dimensionality of the manifold as well as extracting its principal directions for this purpose there are various related techniques including principal component analysis linear discriminant analysis canonical correlation analysis discrete cosine transform random projection etc random projection is a simple and computationally efficient way to reduce the dimensionality of data by trading a controlled amount of error for faster processing times and smaller model sizes the dimensions and distribution of random projection matrices are controlled so as to approximately preserve the pairwise distances between any two samples of the dataset the core idea behind random projection is given in the johnson lindenstrauss lemma which states