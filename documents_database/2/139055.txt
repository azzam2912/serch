sometimes one can very easily construct a very crude estimator g x and then evaluate that conditional expected value to get an estimator that is in various senses optimal the theorem is named after calyampudi radhakrishna rao and david blackwell the process of transforming an estimator using the rao blackwell theorem is sometimes called rao blackwellization the transformed estimator is called the rao blackwell estimator one case of rao blackwell theorem states in other words the essential tools of the proof besides the definition above are the law of total expectation and the fact that for any random variable y e y cannot be less than e y that inequality is a case of jensen s inequality although it may also be shown to follow instantly from the frequently mentioned fact that more precisely the mean square error of the rao blackwell estimator has the following decomposition since formula 4 the rao blackwell theorem immediately follows the more general version of the rao blackwell theorem speaks of the expected loss or risk function where the loss function l may be any convex function if the loss function is twice differentiable as in the case for mean squared error then we