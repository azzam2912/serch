for such problems a necessary condition for optimality is that the gradient be zero newton s method and the bfgs methods are not guaranteed to converge unless the function has a quadratic taylor expansion near an optimum however bfgs can have acceptable performance even for non smooth optimization instances in quasi newton methods the hessian matrix of second derivatives is not computed instead the hessian matrix is approximated using updates specified by gradient evaluations or approximate gradient evaluations quasi newton methods are generalizations of the secant method to find the root of the first derivative for multidimensional problems in multi dimensional problems the secant equation does not specify a unique solution and quasi newton methods differ in how they constrain the solution the bfgs method is one of the most popular members of this class also in common use is l bfgs which is a limited memory version of bfgs that is particularly suited to problems with very large numbers of variables e g 1000 the bfgs b variant handles simple box constraints the algorithm is named after charles george broyden roger fletcher donald goldfarb and david shanno the optimization problem is to minimize formula 1 where formula 2 is