pruning reduces the complexity of the final classifier and hence improves predictive accuracy by the reduction of overfitting one of the questions that arises in a decision tree algorithm is the optimal size of the final tree a tree that is too large risks overfitting the training data and poorly generalizing to new samples a small tree might not capture important structural information about the sample space however it is hard to tell when a tree algorithm should stop because it is impossible to tell if the addition of a single extra node will dramatically decrease error this problem is known as the horizon effect a common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross validation set there are many techniques for tree pruning that differ in the measurement that is used to optimize performance pruning can occur in a top down or bottom up fashion a top down pruning will traverse nodes and trim subtrees starting at the root while a bottom