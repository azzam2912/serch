the first stage finds and encodes large chunks of duplicated data over potentially very long distances 900 mb in the input file the second stage uses a standard compression algorithm bzip2 to compress the output of the first stage it is quite common these days to need to compress files that contain long distance redundancies for example when compressing a set of home directories several users might have copies of the same file or of quite similar files it is also common to have a single file that contains large duplicated chunks over long distances such as pdf files containing repeated copies of the same image most compression programs won t be able to take advantage of this redundancy and thus might achieve a much lower compression ratio than rzip can achieve the intermediate interface between the two stages is made of a byte aligned data stream of which there are two commands a literal add with length and data and a match copy with length and offset parameters literal or match copy lengths of greater than 65 535 bytes are split into multiple instructions end of stream is indicated with a zero length literal add type 0 count 0