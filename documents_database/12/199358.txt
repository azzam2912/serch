given a collection of models for the data aic estimates the quality of each model relative to each of the other models thus aic provides a means for model selection aic is founded on information theory when a statistical model is used to represent the process that generated the data the representation will almost never be exact so some information will be lost by using the model to represent the process aic estimates the relative amount of information lost by a given model the less information a model loses the higher the quality of that model in estimating the amount of information lost by a model aic deals with the trade off between the goodness of fit of the model and the simplicity of the model in other words aic deals with both the risk of overfitting and the risk of underfitting the akaike information criterion is named after the statistician hirotugu akaike who formulated it it now forms the basis of a paradigm for the foundations of statistics as well it is widely used for statistical inference suppose that we have a statistical model of some data let be the number of estimated parameters in the model let formula