it has numerous applications in both science and engineering for example the dynamical system might be a spacecraft with controls corresponding to rocket thrusters and the objective might be to reach the moon with minimum fuel expenditure or the dynamical system could be a nation s economy with the objective to minimize unemployment the controls in this case could be fiscal and monetary policy optimal control is an extension of the calculus of variations and is a mathematical optimization method for deriving control policies the method is largely due to the work of lev pontryagin and richard bellman in the 1950s after contributions to calculus of variations by edward j mcshane optimal control can be seen as a control strategy in control theory optimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved a control problem includes a cost functional that is a function of state and control variables an optimal control is a set of differential equations describing the paths of the control variables that minimize the cost function the optimal control can be derived using pontryagin s maximum principle a necessary condition also known as