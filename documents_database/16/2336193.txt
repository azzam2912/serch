nas has been used to design networks that are on par or outperform hand designed architectures methods for nas can be categorized according to the search space search strategy and performance estimation strategy used nas is closely related to hyperparameter optimization and is a subfield of automated machine learning automl reinforcement learning rl can underpin a nas search strategy zoph et al applied nas with rl targeting the cifar 10 dataset and achieved a network architecture that rivals the best manually designed architecture for accuracy with an error rate of 3 65 0 09 percent better and 1 05x faster than a related hand designed model on the penn treebank dataset that model composed a recurrent cell that outperforms lstm reaching a test set perplexity of 62 4 or 3 6 perplexity better than the prior leading system on the ptb character language modeling task it achieved bits per character of 1 214 learning a model architecture directly on a large dataset can be a lengthy process nasnet addressed this issue by transferring a building block designed for a small dataset to a larger dataset the design was constrained to use two types of convolutional cells to return feature maps