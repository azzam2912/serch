they are particularly suited to streaming data as they adapt to localized changes in the characteristics of the data and don t require a first pass over the data to calculate a probability model the cost paid for these advantages is that the encoder and decoder must be more complex to keep their states synchronized and more computational power is needed to keep adapting the encoder decoder state almost all data compression methods involve the use of a model a prediction of the composition of the data when the data matches the prediction made by the model the encoder can usually transmit the content of the data at a lower information cost by making reference to the model this general statement is a bit misleading as general data compression algorithms would include the popular lzw and lz77 algorithms which are hardly comparable to compression techniques typically called adaptive run length encoding and the typical jpeg compression with run length encoding and predefined huffman codes do not transmit a model a lot of other methods adapt their model to the current file and need to transmit it in addition to the encoded data because both the encoder and the decoder need