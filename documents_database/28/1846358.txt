this is also known as a ramp function and is analogous to half wave rectification in electrical engineering this activation function was first introduced to a dynamical network by hahnloser et al in 2000 with strong biological motivations and mathematical justifications it has been demonstrated for the first time in 2011 to enable better training of deeper networks compared to the widely used activation functions prior to 2011 e g the logistic sigmoid which is inspired by probability theory see logistic regression and its more practical counterpart the hyperbolic tangent the rectifier is the most popular activation function for deep neural networks a unit employing the rectifier is also called a rectified linear unit relu rectified linear units find applications in computer vision and speech recognition using deep neural nets a smooth approximation to the rectifier is the analytic function which is called the softplus or smoothrelu function the derivative of softplus is formula 3 the logistic function the logistic sigmoid function is a smooth approximation of the derivative of the rectifier the heaviside step function the multivariable generalization of single variable softplus is the logsumexp with the first argument set to zero the logsumexp function itself is and its