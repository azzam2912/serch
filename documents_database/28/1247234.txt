the theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters however it does not touch upon the algorithmic learnability of those parameters one of the first versions of the theorem was proved by george cybenko in 1989 for sigmoid activation functions kurt hornik showed in 1991 that it is not the specific choice of the activation function but rather the multilayer feedforward architecture itself which gives neural networks the potential of being universal approximators the output units are always assumed to be linear although feed forward networks with a single hidden layer are universal approximators the width of such networks has to be exponentially large in 2017 lu et al proved universal approximation theorem for width bounded deep neural networks in particular they showed that width n 4 networks with relu activation functions can approximate any lebesgue integrable function on n dimensional input space with respect to formula 1 distance if network depth is allowed to grow they also showed the limited expressive power if the width is less than or equal to n all lebesgue integrable functions except for a zero measure set cannot be approximated by width n