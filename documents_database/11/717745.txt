for example entropy formula 1 is usually defined for discrete random variables whereas for continuous random variables the related concept of differential entropy written formula 2 is used see cover and thomas 2006 chapter 8 both these concepts are mathematical expectations but the expectation is defined with an integral for the continuous case and a sum for the discrete case these separate definitions can be more closely related in terms of measure theory for discrete random variables probability mass functions can be considered density functions with respect to the counting measure thinking of both the integral and the sum as integration on a measure space allows for a unified treatment consider the formula for the differential entropy of a continuous random variable formula 3 with range formula 4 and probability density function formula 5 this can usually be interpreted as the following riemann stieltjes integral where formula 8 is the lebesgue measure if instead formula 3 is discrete with range formula 10 a finite set formula 11 is a probability mass function on formula 10 and formula 13 is the counting measure on formula 10 we can write the integral expression and the general concept are identical in the continuous