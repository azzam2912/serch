spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance originally developed at the university of california berkeley s amplab the spark codebase was later donated to the apache software foundation which has maintained it since apache spark has as its architectural foundation the resilient distributed dataset rdd a read only multiset of data items distributed over a cluster of machines that is maintained in a fault tolerant way the dataframe api was released as an abstraction on top of the rdd followed by the dataset api in spark 1 x the rdd was the primary application programming interface api but as of spark 2 x use of the dataset api is encouraged even though the rdd api is not deprecated the rdd technology still underlies the dataset api spark and its rdds were developed in 2012 in response to limitations in the mapreduce cluster computing paradigm which forces a particular linear dataflow structure on distributed programs mapreduce programs read input data from disk map a function across the data reduce the results of the map and store reduction results on disk spark s rdds function as a working set for distributed programs that offers